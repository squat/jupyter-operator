#!/usr/bin/env bash
set -euo pipefail

GLOG_v=${GLOG_v:-1}

un="$(uname)"
local_os="linux"
if [ ${un} == 'Darwin' ]; then
    local_os="darwin"
fi

# Note: if you increase the number of etcd servers in the Vagrantfile you must also add them here.
etcd_render_flags="--etcd-servers=https://172.17.4.51:2379"

CALICO_NETWORK_POLICY=${CALICO_NETWORK_POLICY:-false}
if [ ${CALICO_NETWORK_POLICY} = "true" ]; then
    echo "WARNING: THIS IS EXPERIMENTAL SUPPORT FOR NETWORK POLICY"
    cnp_render_flags="--network-provider=experimental-calico"
else
    cnp_render_flags=""
fi

# Render assets
if [ ! -d "cluster" ]; then
  ../../_output/bin/${local_os}/bootkube render --asset-dir=cluster --api-servers=https://172.17.4.101:6443 ${etcd_render_flags} ${cnp_render_flags}
  cp user-data.sample cluster/user-data-worker
  cp user-data.sample cluster/user-data-controller
  sed -i -e '/node-role.kubernetes.io\/master/d' cluster/user-data-worker
fi

# Start the VM
vagrant up
vagrant ssh-config c1 > ssh_config

# Copy locally rendered assets to the server
scp -q -F ssh_config -r cluster core@c1:/home/core/cluster
scp -q -F ssh_config ../../_output/bin/linux/bootkube core@c1:/home/core

# Run bootkube
ssh -q -F ssh_config core@c1 "sudo GLOG_v=${GLOG_v} /home/core/bootkube start --asset-dir=/home/core/cluster 2>> /home/core/bootkube.log"

echo
echo "Bootstrap complete. Access your kubernetes cluster using:"
echo "kubectl --kubeconfig=cluster/auth/kubeconfig get nodes"
echo
