#!/usr/bin/env bash
set -euo pipefail

GLOG_v=${GLOG_v:-1}
HOST=${HOST:-c1}

if [ ! -d "cluster" ]; then
  echo "Need some cluster assets to perform recovery; try running bootkube-up."
fi

if [ ! -f cluster/bootstrap-manifests/bootstrap-etcd.yaml ]; then
  echo "ERROR: run bootkube-test-recovery for non self-hosted etcd Kubernetes cluster."
  exit 1
fi

echo "Getting the etcd backup file"
echo

ssh -q -F ssh_config core@$HOST "sudo cp /var/etcd/kube-system-kube-etcd-0000/member/snap/db /home/core/etcdbackup"
ssh -q -F ssh_config core@$HOST "sudo chown core:core /home/core/etcdbackup"
scp -q -F ssh_config core@$HOST:/home/core/etcdbackup cluster/etcdbackup 

echo
echo "Destroying and re-creating the master node..."
echo

vagrant destroy -f $HOST
vagrant up $HOST

echo
echo "As you can see, the cluster is now dead:"
echo

set -x
! kubectl --kubeconfig=cluster/auth/kubeconfig get nodes
{ set +x; } 2>/dev/null

echo
echo "Recovering the control plane from the etcd backup..."
echo

scp -q -F ssh_config ../../_output/bin/linux/bootkube cluster/auth/kubeconfig cluster/etcdbackup core@$HOST:/home/core
ssh -q -F ssh_config core@$HOST "sudo GLOG_v=${GLOG_v} /home/core/bootkube recover \
  --recovery-dir=/home/core/recovered \
  --etcd-backup-file=/home/core/etcdbackup \
  --kubeconfig=/home/core/kubeconfig 2>> /home/core/recovery.log"

echo
echo "Running bootkube start..."
echo

ssh -q -F ssh_config core@$HOST "sudo GLOG_v=${GLOG_v} /home/core/bootkube start --asset-dir=/home/core/recovered 2>> /home/core/recovery.log"

echo
echo "The cluster should now be recovered. You should be able to access the cluster again using:"
echo "kubectl --kubeconfig=cluster/auth/kubeconfig get nodes"
