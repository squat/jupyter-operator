#!/usr/bin/env bash
set -euo pipefail

GLOG_v=${GLOG_v:-1}

un="$(uname)"
local_os="linux"
if [ ${un} == 'Darwin' ]; then
    local_os="darwin"
fi

SELF_HOST_ETCD=${SELF_HOST_ETCD:-false}
if [ ${SELF_HOST_ETCD} = "true" ]; then
    echo "WARNING: THIS IS NOT YET FULLY WORKING - merely here to make ongoing testing easier"
    etcd_render_flags="--experimental-self-hosted-etcd"
else
    etcd_render_flags=""
fi

CALICO_NETWORK_POLICY=${CALICO_NETWORK_POLICY:-false}
if [ ${CALICO_NETWORK_POLICY} = "true" ]; then
    echo "WARNING: THIS IS EXPERIMENTAL SUPPORT FOR NETWORK POLICY"
    cnp_render_flags="--experimental-calico-network-policy"
else
    cnp_render_flags=""
fi

# Render assets
if [ ! -d "cluster" ]; then
  ../../_output/bin/${local_os}/bootkube render --asset-dir=cluster --api-servers=https://172.17.4.100:443 ${etcd_render_flags} ${cnp_render_flags}
  cp user-data.sample cluster/user-data
	if [ ${SELF_HOST_ETCD} = "false" ]; then
		cat user-data-etcd.sample >> cluster/user-data
	fi
fi

# Start the VM
vagrant up
vagrant ssh-config > ssh_config

# Copy locally rendered assets to the server
scp -q -F ssh_config -r cluster core@default:/home/core/cluster
scp -q -F ssh_config ../../_output/bin/linux/bootkube core@default:/home/core

# Run bootkube
ssh -q -F ssh_config core@default "sudo GLOG_v=${GLOG_v} /home/core/bootkube start --asset-dir=/home/core/cluster 2>> /home/core/bootkube.log"

echo
echo "Bootstrap complete. Access your kubernetes cluster using:"
echo "kubectl --kubeconfig=cluster/auth/kubeconfig get nodes"
echo
